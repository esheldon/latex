\documentclass[12pt]{article}

\usepackage{graphicx}

\usepackage[margin=1in,dvips]{geometry}

\usepackage{deluxetable}

\newcommand{\commissdate}{Fall 2012}
\newcommand{\surveyproper}{Spring 2013}
\newcommand{\devauc}{De Vaucouleurs'}
\newcommand{\devprof}{exp$(-r^{1/4})$}
\newcommand{\overhead}{10\%}

% Get rid of the normal page numbers
%\usepackage{nopageno}

% Now use fancyhdr but only to clear the headers and put a page number in the
% upper right

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\rhead{}
%\rfoot{}
%\lfoot{}
%\cfoot{\thepage}
%\renewcommand{\headrulewidth}{0.0pt}

% for comments
\usepackage{verbatim}


% bibliography stuff
\usepackage{natbib}
\input{aasdef.tex}

\begin{document}


\addcontentsline{toc}{section}{Cover Page}
%\addcontentsline{toc}{section}{Table of Contents}

% Title and TOC
%\newpage 

\begin{center}
 {\large {\bf Cover Page}}
\end{center}

\vspace{3mm}
\noindent
\begin{center}
{\bf LAB 12-751 Early Career:  \\ Measuring Dark Energy with Gravitational \\
Lensing in the Dark Energy Survey}
\end{center}

\vspace{3mm}
\noindent
{\bf DOE National Laboratory:} Brookhaven National Laboratory

\vspace{3mm}
\noindent
{\bf Street Address:} PO Box 5000, Upton, NY 11973

\vspace{3mm}
\noindent
{\bf Principal Investigator (PI):} Erin Sheldon

\vspace{3mm}
\noindent
{\bf Position Title of PI:} Physicist

\vspace{3mm}
\noindent
{\bf Business Mailing Address of PI:} Bldg 510, Brookhaven National Laboratory, Upton, NY
11973

\vspace{3mm}
\noindent
{\bf Telephone Number of PI:} (631) 344-3117

\vspace{3mm}
\noindent
{\bf Email of PI:} erin.sheldon@gmail.com

\noindent
{\bf Program Announcement: }LAB 12-751

\vspace{3mm}
\noindent
{\bf DOE/Office of Science Program Office:}  HEP

\vspace{3mm}
\noindent
{\bf Topic Area:} Cosmic Frontier, Experimental High Energy Physics Research

\vspace{3mm}
\noindent
{\bf Topic Area Program Manager:} James Stone

\vspace{3mm}
\noindent
{\bf Year Doctorate Awarded:}  2002


\vspace{3mm}
\noindent
{\bf Number of Times Previously Applied:} 2

\vspace{3mm}
\noindent
{\bf PAMS Preproposal Number:} 857


\vspace{3mm}
\noindent
{\bf PECASE Eligible:} Yes



\newpage

\tableofcontents

\newpage

\title{Early Career: Measuring Dark Energy with Gravitational Lensing in 
the Dark Energy Survey}
\author{Erin Sheldon\\
{\normalsize Physicist}\\
\normalsize{Brookhaven National Laboratory}}
\date{}
\maketitle



\addcontentsline{toc}{section}{Abstract}
\begin{center}
\section*{Abstract}
\end{center}

%\begin{abstract}

I propose to analyze data from the Dark Energy Survey (DES) to constrain the
properties of Dark Energy.  My primary focus will be on measuring gravitational
lensing effects to probe the expansion history and growth rate of massive
structures in our universe.  I am a DES ``builder'' with data rights and a
leader of the DES lensing effort.

Dark Energy accelerates the expansion of the universe, dramatically increasing
the volume in comparison to a matter-only universe.  Dark Energy also inhibits
the growth of massive structures under gravitational collapse.  Thus the number
density of massive objects such as galaxy clusters as a function of cosmic time
is directly related to the properties of Dark Energy, in particular the
equation of state parameter $w=$pressure/density.  Critical to using the number
density to constrain cosmology is knowing the masses of the clusters. I will
measure these masses using gravitational lensing.  Using cluster counts,
lensing and other complimentary probes, the DES will measure $w$ to $\sim$3\%.
This program is a natural continuation of my earlier measurements of cluster
lensing in the Sloan Digital Sky Survey (SDSS), which are most sensitive such
measurements to date, and naturally leads to future work on projects such as
the Large Scale Synoptic Telescope\cite{lsstweb}.

DES has entered the commissioning phase.  I am finishing the prototyping stage
of a promising new algorithm to measure lensing effects from astronomical
images.  This method shows potential to meet the DES requirements for lensing
measurement accuracy. In addition, in collaboration with Mike Jarvis of UPenn,
I am continuing to test and tune the de-facto algorithm and pipeline for DES.
We are strenuously testing these algorithms using simulations and will soon
begin testing on the commissioning data.

After first light, DES will take data for five years, during which I will
process the data as it arrives and perform analysis to extract Dark Energy
parameters.  These analyses will most likely produce results in three stages:
an early set of results from the first year data, a second set of results after
two or three years when multi-epoch data are available, and a final set of
results from the full data set circa 2017.  The DES survey will produce of
order a petabyte of data which will arrive in a steady stream.  For this work I
will require moderate computing infrastructure and the assistance of a research
associate.

%\end{abstract}

\newpage
\addcontentsline{toc}{section}{Narrative}
\section*{Narrative}
\setcounter{section}{1}
\subsection{Introduction}

%The phenomenon of Dark Energy is one of the unsolved mysteries of our time.
The initial discovery of Dark Energy was made by studying the expansion history
of our universe.  According to General Relativity, a universe containing only
ordinary matter decelerates at late times under its own gravitation, but recent
studies of the brightnesses of distant supernovae indicate that our universe
has begun to accelerate \cite{Riess98,Perlmutter99}.  This can happen if there
is an exotic energy component in our universe with an equation of state
parameter $w$=pressure/density that is less than -1/3.

The observational consequences of a Dark Energy component in our universe are
numerous, but for our purposes the following are most relevant: 1) The
expansion history follows a form quite different from that of a matter only
universe, with a change in the sign of the acceleration at late times from
negative to positive \cite{Carroll92}.  2)  Unlike in a matter-only universe,
the growth history of massive structures through gravitation is no longer
determined solely by the properties of the mass density field and the value of
the present day expansion rate \cite{Haiman01}.  For example, without Dark
Energy, the number density of gravitationally collapsed structures of a given
mass at a given time in history is predictable essentially from the Hubble
expansion constant $H_0$ and low order statistics of the mass field such as the
mean of the density and the variance in the density as a function of scale
(ignoring baryons).  But in the presence of Dark Energy, the volume changes
over time in a dramatically different way and the growth of structures is
inhibited.  The cumulative effects of Dark Energy significantly alter the
predicted number of massive structures in a given volume of space.

Gravitational lensing is particularly well suited to studying this problem
(e.g. \cite{Kaiser98,Hu04}).  Lensing is the apparent bending of light as it
passes massive structures.  The amount of bending depends on the lens mass and
the geometry of the lens-source-observer system.  Thus, measurements of a large
sample of lenses at various cosmological epochs will elucidate the expansion
history of the universe, encoded in the geometry, and the growth history,
encoded in the statistical distribution of the measured masses.  

Lensing is more appropriate for measuring masses in an cosmological context
than other techniques.  The traditional technique of using orbital calculations
and Kepler's laws to infer masses does not work because the timescales are too
long to characterize the orbits. Velocities can only be used in a statistical
way, and require assumptions about the dynamical equilibrium of the systems in
question, which is often dubious on the physical scales of interest.
Furthermore, interpretation based on luminous tracers is complicated by the
ubiquitous presence of another mysterious substance: Dark Matter.  Dark Matter
dominates the mass density field, but because it is collisionless it is
distributed very differently from the luminous matter.  Often there are no
luminous tracers in the relevant regions of space with which to infer masses.
With lensing, one only needs enough background sources with which to
statistically measure the lensing signal.

I propose to use data from the Dark Energy Survey (DES, \cite{DESWhitePaper})
to perform gravitational lensing measurements and infer the properties of Dark
Energy.  I am a DES ``builder'', having worked steadily on the project since
it's inception.  Among other things, this designation brings perpetual data
rights for myself, a postdoc and students.  I am a leader in the DES lensing
effort.  My focus will be primarily on optimal measurement of the gravitational
shear induced in the shapes of galaxies by lensing, and using the shear to
infer statistics of the mass density field, namely the mass associated with
galaxy clusters and the power spectrum of the mass density fluctuations in our
universe. Both of these measurements will be highly sensitive to the properties
of Dark Energy.  Using a combination of techniques, we expect to constrain the
equation of state parameter $w$ to a few percent.  As I will describe below,
this is a natural extension of my earlier work in Gravitational Lensing using
data from the Sloan Digital Sky Survey (SDSS) and will lead naturally to work
on the Large Scale Synoptic Telescope \cite{lsstweb}, in which I am a
participant.

The rest of this narrative is divided as follows: In section \ref{sec:sdssold}
I describe my previous research in gravitational lensing, which can be thought
of as a precursor for the DES work.  In section \ref{sec:des} I describe the
Dark Energy Survey (DES) and in section \ref{sec:deslensing} I describe my
plans for lensing measurements using DES data.  In section \ref{sec:gmix} I
describe the new technique for shear measurement I am developing, which
shows promise to reach the required precision for DES, and in section
\ref{sec:shapelets} I describe the ``de-facto'' pipeline we have developed over
the past few years.  Finally, in sections \ref{sec:resources} and
\ref{sec:timeline} I describe the personnel and resources needed to complete the
work in this proposal and the expected timeline.

\subsection{Previous Lensing Measurements in the SDSS} \label{sec:sdssold}

Using data from the SDSS \cite{York00}, I have made highly accurate and precise
measurements of gravitational shear. I have used these measurements to estimate
the total mass content (normal and dark) associated with galaxies and clusters
of galaxies
\cite{fis00,Sheldon04,SheldonLensing07,JohnstonLensing07,SheldonM2L07}.  These
high quality measurements are facilitated by the excellent data and processing
software of the SDSS, and our development of interpretational techniques that
can extract masses from complex statistical shear measurements
\cite{JohnstonInvert07}.

In these works we measured the shear from millions of background source
galaxies at various projected distances from foreground lenses, from which we
inferred the radial mass density profile.  Because the signal is very weak, we
additionally averaged the signal over many lenses.  This averaging, while in
principle diluting information about the individual lenses, has the positive
effect of averaging out line of sight projections and ``lumpiness'' in the
lenses, which complicates the interpretation.  This in turn facilitates the
extraction of accurate masses.  Figure \ref{fig:massngals} shows results
from \cite{SheldonLensing07,JohnstonLensing07}. Plotted is mean cluster mass as
a function of the number of galaxies in the cluster.  

%\begin{figure}[ht]
\begin{figure}[p]
\centering
\includegraphics[scale=0.7]{mass-rich-plot.eps}
\caption{Mean cluster mass as a function of the number of
galaxies in the cluster as measured from lensing in SDSS
data \cite{SheldonLensing07,JohnstonLensing07}. This calibration
is critical to measuring Dark Energy with galaxy clusters.\label{fig:massngals}}
\end{figure}



%\begin{figure}[ht] 
\begin{figure}[p] 
\centering 
\includegraphics[scale=0.6]{s8_Om.ps}

\caption{Constraints on the fractional mass density of our universe $\Omega_m$
and the relative variance in the density on 8 Mpc scales $\sigma_8$ as measured
from SDSS data.  These results \cite{RozoCosmo09} are derived by combining the
counts of galaxy clusters with the mass calibrations from gravitational lensing
as shown in Figure \ref{fig:massngals}
\cite{SheldonLensing07,JohnstonLensing07}.  The cluster results break
degeneracies with other probes such as the cosmic microwave background (WMAP).
With DES we will study Dark Energy properties by extending these measurements
back in time.  \label{fig:omegasigma8}} \end{figure}



From galaxy and cluster lensing measurements we confirmed that there is an
enormous amount of unseen dark matter in galaxies, and that this dark matter is
in a ``dark halo'' that extends far beyond the concentrated bundle of stars at
the center of galaxies.  These measurements are completely consistent with the
cold dark matter model.  A number of derived results have come from these basic
measurement papers, in which we have learned a great deal about the connection
between the dark and visible matter in galaxies and clusters, e.g.
\cite{RykoffLXM08,RozoScatter09,TinkerM2N2012}. 

We have also used these measurements to estimate cosmological parameters.  As
stated in the introduction, the number density of halos of a given mass is
related to the mean mass density of the universe and variance in the density.
In \cite{RozoCosmo09} we combined the counts of galaxy clusters with our
lensing mass estimates to constrain these cosmological parameters.  Figure
\ref{fig:omegasigma8} shows results from \cite{RozoCosmo09} constraining the
fractional mass density $\Omega_m$ and the relative variance in mass density on
8 Mpc scales $\sigma_8$.  In \cite{TinkerM2N2012} we use the ratio of mass to
number density in clusters, combined with the large scale clustering of all
galaxies, to place further complimentary constraints on $\Omega_m$ and
$\sigma_8$.  All these results are consistent with one another and the Cold
Dark Matter theory.

While powerful in themselves, these results are also very complimentary to
other measurements, breaking degeneracies in analyses of the Cosmic Microwave
Background \cite{KomatsuWMAPCosmo09}. 

As I will describe in the section \ref{sec:des} on DES, gravitational shear
measurements are central to two of that survey's primary goals.  The techniques
we developed in the SDSS are directly applicable to DES science, especially the
study of galaxy clusters as cosmological probes.  By extending the measurements
backward in time with the deeper DES data, we will learn about Dark Energy as
well as Dark Matter.


\subsection{The Dark Energy Survey (DES)} \label{sec:des}

The Dark Energy Survey (DES) is an optical, multi-band survey of 5000 square
degrees using the 4-meter ``Blanco'' telescope at the Cerro Tololo
Inter-American Observatory in Chile. A new camera is being built and the
telescope repaired and upgraded.  The DES will utilize gravitational lensing,
an optical cluster survey, supernovae, and galaxy clustering to constrain the
properties of Dark Energy.  Combining DES lensing measurements and DES optical
observations of galaxy clusters with observations by the South Pole Telescope
(SPT, \cite{SPT04}) of the same galaxy clusters, greatly enhances the
constraining power.  These combined methods will constrain the Dark Energy
equation of state parameter $w$ to better than 3\%.  First light occured in
\commissdate, and the survey will run for five years.  DES operations are
funded in part by the U.S.  Department of Energy. 


The SPT will use the Sunyaev-Zel'dovich (SZ) Effect \cite{Birkinshaw99}, the
Compton up-scattering of light from the cosmic microwave background by the hot
gas in galaxy clusters, to find a complete sample of clusters to high redshift.
This cluster sample has selection that is complimentary to the DES optical
cluster selection in that it is expected to be nearly independent of the
distance of the cluster from the observer.  As with DES, the goal of the SPT is
to use these clusters to probe the growth of structure, and the volume of
space, as a function of time in order to constrain Dark Energy.  Since it is
the number density of clusters of a given mass that is sensitive to Dark
Energy, an important part of each cluster survey will be the calibration of the
mass-observable relationship via lensing.  In \S \ref{sec:deslensing} I will
describe in detail our plans for measuring this relationship using DES optical
data.

In addition to galaxy clusters, the DES will use a number of other probes to
constrain Dark Energy properties.  These include two other lensing probes:
Shear-shear correlations as a function of scale and the cross-correlation
between shear and known objects as a function of scale.  Data are shared
between cluster mass measurements and these probes, but because the correlation
functions cover a much larger range of scales, they are complimentary.  There
is also a Supernova program that, while less constraining by itself, breaks
degeneracies between certain cosmological parameters.  

Table \ref{table:constraints} shows forcasted constraints on $w$ for various
techniques employed by DES \cite{DESWhitePaper}.  These forecasts are for DES
and SPT data alone; combining with other data, for example from cosmic
microwave background measurements from the Planck satellite
\cite{PlanckBluebook}, can significantly increase the precision of certain
probes.

\begin{deluxetable}{ll}
\tablecaption{Projected DES Constraints on Constant $w$ Dark Energy Models.
\label{table:constraints}}
\tablewidth{0pt}
\tablehead{
	\multicolumn{1}{l}{Method} &
	\colhead{$\sigma_w$}
}
\startdata
Clusters &  \\
~~~Abundance & 0.13  \\
~~~with WL Calibration & 0.09 \\
Weak Lensing & \\
~~~Cosmic Shear (CS) & 0.15  \\
~~~Galaxy/Cluster-shear(GS) + Angular Clustering(AC) & 0.08  \\
~~~CS + GS + AC & 0.03  \\
Angular Clustering of Galaxies & 0.36 \\
Supernovae Ia & 0.34 \\
\enddata
\end{deluxetable}



\subsection{Lensing Analysis of DES Data} \label{sec:deslensing}

\subsubsection{Science Analysis}

As described in the introduction, the cosmological information from clusters is
primarily in the number density of clusters with a given mass as a function of
time.  Clusters are identified not by their mass but by other indicators, such
as the SZ effect from SPT data or by the clustering of visible galaxies in DES
imaging data.  The strength of the SZ effect and the number of galaxies are
both correlated with mass, but that correlation must be measured by a secondary
method.  As described in the introduction, lensing is the best method for doing
this.  


I will process the data data to calibrate the masses of the galaxy clusters
used in the cosmological analyses.  I will also participate in the measurement
of shear-shear correlations (cosmic shear).  These are critical components of
the DES mission.  


As described in \S\ref{sec:sdssold}, in our studies of SDSS lensing we have
developed analysis techniques to calibrate the mass-observable relation of
clusters (Figure \ref{fig:massngals}).  Using this calibration in conjunction
with the number density we have inferred cosmological parameters (Figure
\ref{fig:omegasigma8}).  These techniques are limited only by our understanding
of the systematics and characterization of the cluster selection process.  The
volume and depth of DES is sufficiently large to perform equivalent
measurements in many bins of cosmic time.  Time dependent measurements will
allow us to extend our cosmological analysis to constrain the properties of
Dark Energy.

In contrast with cluster lensing measurements, cosmic shear is the correlation
of shears across the sky independent of the location of foreground structures.
Since the shear is related to mass, the cosmic shear can be used to directly
infer statistics of the underlying mass distribution, the evolution of which is
directly related to the properties of Dark Energy.  Because the signal need not
be modeled in terms of cluster halos, the interpretation of cosmic shear can be
simpler than cluster lensing.  However, the measurement involves directly
correlating shears from many sources as a function of their separation on the
sky, which can propagate systematic errors directly into the measurement. Thus
cosmic shear and cluster lensing are quite complimentary.


Table \ref{table:constraints} shows the power of lensing in the DES to
constrain $w$ as compared to other DES probes.

%\lhead{}
\subsubsection{Data Reduction and Removal of Systematic Effects} 
\label{sec:des:process}

Gravitational Shear alters the shapes of galaxy images, producing recognizable
patterns in their ellipticities across the sky.  As described in \ref{sec:gmix},
these patterns are hidden by the blurring by the atmosphere and telescope
optics, and the images must be corrected.

I have created a generic data processing pipeline to analyze DES images, and
analysis codes to remove the PSF effect and measure lensing using that
framework.  At this time we have a full shear measurement pipeline based on the
shapelets technique and I am developing a new algorithm based on Gaussian
mixtures.  These techniques are described in sections \ref{sec:shapelets} and
\ref{sec:gmix} respectively.

\subsection{New Lensing Shear Measurement Technique Using Gaussian Mixtures}
\label{sec:gmix}

The weak gravitational shear effect discussed in the previous sections is a
very small distortion of the images of galaxies by foreground mass
distributions due to the bending of light.  This small distortion produces
correlations in the ellipticities of the background galaxies, which can only be
measured statistically.  In principle all one needs is enough galaxies to
extract this signal from the noise.  However, the telescope optics and
atmosphere distort and blur the images of galaxies, and this also produces
correlations in the ellipticities of galaxies; the combined effect is generally
referred to as the point spread function (PSF). The primary affect is a
convolution, which means the effect is more significant for smaller galaxies.
Most detected galaxies are of order the size of the PSF, and the effect is
typically at a level much larger than the lensing effect we wish to measure.

Many methods have been developed to measure and remove the effect of the PSF to
produce unbiased ellipticity measurements, e.g.
\cite{ksb95,Bern02,Miller07,Melchior11} and many more.  No method yet tested
has proven accurate enough to meet the DES survey requirements for realistic
galaxy populations:  for DES to measure the projected constraints on
dark energy, the fractional error on the shear cannot exceed 0.004.

I have been developing a new technique that shows promise to meet the DES
requirements.  The method uses mixtures of gaussians to represent the galaxies
and PSF, and bayesian techniques to control noise bias.  This model has enough
flexibility to represent the canonical galaxy types; this has recently been
independently verified \citep{HoggGMix12}.  One result of my recent
research is that many fewer gaussians are required than used in
\cite{HoggGMix12} when the object is of order the size of the PSF; three is
enough.  When also demanding the Gaussians are co-centric and co-elliptical the
parameter space is reduced even further.  

It is tempting to just use the canonical galaxy profiles (exponential disks and
\devprof\ ``\devauc'' profiles) to fit for the ellipticity, but these are
computationally intensive to generate when convolving with a PSF and the pixel
response.  Using Gaussians for both PSF and galaxy greatly increases the speed
because the convolutions can be done analytically.  This is important for
controlling the noise bias described below.  Furthermore, the Gaussian mixture
model can adapt to profiles that differ from the canonical models.

Other features of the method are more generic and are designed to mitigate the
effects of noise.  Noise is known to produce bias in the shear measurement when
using either maximum likelihood \cite{Refreg12} or the full expectation value
\cite{Miller12}.  This can be mitigated using the techniques in
\cite{Miller07,Miller12}, which involve using prior information on the
ellipticity and size distributions of the galaxy population as a whole.  Use of
the prior on ellipticity was introduced in \cite{Miller07}. I developed the use
of the size prior distribution and this was recently also used by other
researchers \cite{Miller12}.  I have implemented these techniques using a Monte
Carlo Markov Chain technique, and the code is publicly available (although the
method is clearly described in \cite{Miller07}, that author's code is
proprietary).

The results from simulations indicate the method can potentially meet DES
requirements.  For exponential disk galaxies, a prior on the ellipticity
distribution is sufficient, as shown in the left panel of figure
\ref{fig:getgdt}.  The DES requirements for shear accuracy are met for galaxies
$\geq$ the size of the PSF and for signal-to-noise ratios $\geq$ 20.  For
\devprof\ galaxies a prior in the size distribution is also required for
unbiased shear measurements, as shown in the right panel of figure
\ref{fig:getgdt}.  More simulations are needed to fully characterize the error,
but it appears that with the size prior in place the accuracy will be
sufficient.

\begin{figure}[t]
\centering

\includegraphics[scale=0.4]{mcbayes-get01r05r06r07r08-yr-0.050-0.050-frac.eps}
\includegraphics[scale=0.4]{mcbayes-gdt02r11-yr-0.050-0.050-frac.eps}

\caption{Fractional shear bias as a function of signal-to-noise ratio (S/N).
Left panel: Exponential disk galaxies fit by a Gaussian mixture.  The PSF
represents atmospheric turbulence.  The different line/point styles represent
galaxies of different size ratio with respect to the point spread function.  A
prior on the distribution of ellipticities has been applied.  The gray band is
the DES requirement on the shear error; the error is within the DES
requirements for S/N $\geq$ 20 and for galaxies comparable to or larger than
the PSF.  Right panel: \devprof\ galaxies fit by a Gaussian mixture with the
same PSF as the left panel. In addition to a prior on ellipticity, a prior on
the distribution of sizes was applied.  Measurements of these galaxy types are
intrinsically more noisy than exponential disks, and at the current writing
more simulations are needed to fully characterize the error, but with the
current statistics the error is consistent with the DES requirements. 
A topic of future research will be to
determine the priors from data 
\label{fig:getgdt}}

\end{figure}

\subsubsection{Future Work on Gaussian Mixtures}

A focus of future work on this method will be determining the priors on
ellipticity and size from real data.  This is a topic of research because, at
faint magnitudes, it is difficult to determine these distributions, especially
for \devprof\ galaxies\cite{Miller12}.  Another area of focus will be testing
the method on more complex galaxy and PSF models, and testing the method on
real data.  This is a significant challenge both in terms of man power and
compute power. 

\subsubsection{Speeding Up the Computations with GPUs} \label{sec:gmix:gpu}

Correction for noise effects requires measurement of the full likelihood
surface, and is thus very computationally intensive.  Using the Gaussian
mixture model provides a fast and flexible model for the necessarily
convolutions.   While relatively fast, about 1 second per galaxy, the
requirements for DES are enormous: the expected 500 million galaxies will be
observed on $\sim$ 10 separate exporues, which would require $\sim$ 1.4 million
cpu hours to process.

Great gains can made by using graphics processing units (GPUs).  Using my own
GPU implementation I was able to increase the speed by a factor of $\sim 8$ for
an individual likelihood calculation. Many can be run simultaneously on a
single GPU, depending on the available memory and architecture.  On typical
scientifically oriented GPUs (I tested a nvidia 2050) one has sufficient
resources to run $\sim$ 20 likelihood computations at a time with no
significant overhead; this is the desired number of ``parallel walkers'' when
using an affine invariant Markov chain sampler, so is well suited to our needs.
One can attach 2 GPUs to an individual compute node.

These increases in speed come at a moderate increase in cost over typical
``farm'' compute nodes.  Recent purchases for the Atlas experiment at
Brookhaven National Laboratory we estimate the cost to be about \$5,200 (32GB
memory, 13TB disk, 12 cores).  The GPUs I have tested (nvidia 2050) can be
purchased for $\sim$ \$1,800.  If we assume an increase in speed of 8 per
likelihood calculation, and 40 such calculations running simultaneously over 2
separate GPUs, this is a factor of 26 increase in speed over the 12 cpu cores
on those systems for less than a doubling of the total cost.  In section
\ref{sec:computing} we outline the moderate cost to build a sufficient system.

Notes: some more time can be spent tuning the GPU implementation, but my tests
indicate it will be difficult to improve on the above speedups due to memory
bandwidth limitations.  Also note a high performance distributed file system is
needed to get the required data throughput.  The cosmology group at BNL uses
the Hadoop Distributed File System, which provides sufficient throughput.


\subsection{Shear Measurement Using Shapelets}
\label{sec:shapelets}

In addition to the Gaussian mixture method described in section \ref{sec:gmix},
Mike Jarvis of the University of Pennsylvania and I have implemented a full
shear pipeline based on the shapelets technique\cite{Bern02}.   The accuracy
does not yet meet DES requirements, but this pipeline is mature in the sense
that it can now run fully autonomously on the data and is fast and stable.
Tuning of the pipeline to increase accuracy is being led by Jarvis and is
showing promise.   The ideas for reducing bias, outlined in section
\ref{sec:gmix}, can in principle be applied to this technique as well.

This pipeline can make full use of the multi-epoch DES data where the sky is
observed many times.  This code has been tested on simulated DES data and will
continue to undergo heavy testing and refinement throughout the commissioning
phase.  After the survey comes online in \surveyproper, we will process the
data in real time as it arrives.  The processing of simulated and real data
will require significant manpower and computing resources.

The processing pipeline was implemented by myself.  This pipeline is
``pluggable'' so that other codes can be plugged in readily and can make use of
the infrastructure built around the shapelets code.  I plan to plug in
the Gaussian mixture code as well as 2-3 other pipelines developed within
DES.

Note the shapelets pipeline, as it is implemented currently, does not gain
significantly by running on GPUs because it is a maximum likelihood technique
and so requires relatively few evaluations.  If the bias-correction schemes
discussed in section \ref{sec:gmix} are implemented for shapelets then
gains can be had using GPUs.

\subsection{Personnel and Resources} \label{sec:resources}

\subsubsection{Research Associate}

We plan for much of the higher level analysis and many of the science papers to
be led by research associates (postdocs).  We expect to hire a postdoc during
the first year of the award for three years and a second postdoc to begin in
the fourth year.  The work of these postdocs will focus on testing the shear
pipelines, measurement of shear statistics and interpretation in terms of mass
and cosmology.  These analyses will be important and will have high scientific
impact.

In addition, as we plan to run multiple shear measurement pipelines, we expect
a postdoc to take ``ownership'' of at least one of these pipelines to ensure
the processing goes smoothly and to implement tests.  An associate with
experience using GPUs would be ideal, but this is certainly not a requirement
as the technique can be learned readily.

\subsubsection{Computing} \label{sec:computing}

We are commissioning the DES survey at the time of this writing (\commissdate)
and will run for five years, generating about a petabyte of data in total.  For
lensing we will process all the reduced single-epoch images and multi-epoch
images, which amounts to about 130 terabytes.

The NSF funded Dark Energy Survey data management team will process the data
using a selected lensing pipeline approximately yearly.  The DES may also have
some computing available to run the shapelets pipeline, but this will not be a
GPU based system so the still-immature Gaussian mixture code is not a good use
of that resource.  The Gaussian mixture pipeline is still in heavy development
and will require many runs on the data and simulations to find and fix bugs.  A
turnaround time of 2.5 weeks is desirable.

As described in \ref{sec:gmix:gpu}, we want to process all 500 million
galaxies, each on 10 separate exposures, in this time of 2.5 weeks. This would
require about ten nodes with two GPUs each.  We desire fairly large memory on
the system in order to hold data from all exposures in memory for a full
``coadd tile''. A coadd tile is the unit of association used when combining
exposures and covers several tens pointings, with about 10 exposures per
pointing.  Tests indicate 32GB is sufficient to avoid input/output overhead.
Finally, we need 130TB in the distributed file system, so about 13TB per
machine (we can in principle do without redundancy as the data are readily
available for download from the DES collaboration).  Recent purchases for the
Atlas experiment at BNL are very similar to this and we project a cost of
approximately \$5,200.  The addition of two GPUs adds another $\sim$ \$3,600.
The total cost for ten machines is \$88,000 plus overhead.  These can be
purchased over the five year survey.  There is potential for increase in speed
per GPU, but this is difficult to predict currently as scientific GPUs are a
new technology.  I prefer to quote the current performance profile.

Note these prices include bulk discounts from purchasing along with other
experiments through the RHIC-Atlas Computing Facility at BNL.  

\input{cost.tex}

\subsubsection{Salary and Travel}

It is required that DOE employees pay their salary from this grant, so 100\% of
Erin Sheldon's salary and overhead for five years is requested.  I also request
100\% for a research associate for five years.

The remaining funds are beyond salary and computing is primarily for travel
expenses.   The DES collaboration is multi-national and Erin Sheldon and a
postdoc will most likely travel abroad to the collaboration meetings.  Sheldon
plans to spend three weeks each summer at a workshop in the US, such as those
held at Aspen and Santa Fe.  Further travel will involve attending a few
conferences per year across the US for both Sheldon and postdoc.  Erin Sheldon
will also make regular trips to the University of Pennsylvania to collaborate
with Mike Jarvis and Bhuvnesh Jain.

\clearpage
\newpage
\subsection{Timeline} \label{sec:timeline}

This is an outline of the activities for the five year extent of the award.  It
is assumed that the primary collaborators on these activities are Erin Sheldon
(ES) of BNL, current and research associates (RA) to be hired in the Fall of 2013
and 2016.

\begin{itemize}

\item {\bf Fall 2012-Spring 2013} Process and evaluate commissioning data as it
arrives  (ES,RA) Continued testing of the weak lensing pipelines on simulated
DES data (ES, RA).

\item {\bf Late Spring through end of 2013} Survey proper begins; process data
as it arrives.  Re-process data as bugs are found and algorithms improve
(ES,RA).  Processing using Gaussian mixtures at BNL on GPU cluster.  Begin
science lensing analysis (ES,RA).

\item {\bf 2014} Continue processing data and testing pipelines.  Publication
of first year DES measurements for galaxy and cluster-mass correlations
(ES,RA).

\item {\bf 2015} Continue processing data as it arrives (ES,RA).  Continue
improving algorithms (ES,RA).   First publications using multi-epoch data
(ES,RA) in 2014.

\item {\bf 2016-2017}  Activities should continue as before until survey end.
New RA will arrive in Fall 2016.  Processing data as it arrives, incrementally
improving the data pipelines and analysis codes.  Analysis methods will evolve,
especially as the final data are in hand and there are many epochs with which
to work.   There will be intermediate publications based on this data.

\item {\bf Post survey} A re-processing of all data through the final pipelines
and final analysis of the full dataset to extract cosmological information
(ES,RA).


\end{itemize}







\newpage
\addcontentsline{toc}{section}{Appendix 1: Biographical Sketch}
\section*{Appendix 1: Biographical Sketch}

%\setlength{\oddsidemargin}{-0.1in}
%\setlength{\evensidemargin}{-0.1in}
%\setlength{\textwidth}{6.7in}
%\setlength{\topmargin}{-0.25in}
%\setlength{\textheight}{9.0in}

\newcommand{\tsp}{\vspace{0.1cm}}
\newcommand{\isp}{\vspace{0.3cm}}
\newcommand{\ssp}{\vspace{0.4cm}}


{\Large {\bf Erin Sheldon}}
\tsp
%
% Address information.
%

\noindent
Bldg 510

\noindent
Brookhaven National Laboratory

\noindent
Upton, NY 11973

\noindent
(631) 344-3117

\noindent
erin.sheldon@gmail.com

\newcommand{\myshorttab}{0.5in}
\newcommand{\mytab}{0.75in}
\newcommand{\mylongtab}{1.25in}

\vspace{0.4cm}
\noindent
\makebox[1.25in][l]{{\large \bf Education and Training}}{}
\vspace{0.2cm}
\newline
\makebox[\mytab][l]{}{Postdoctoral Fellow, Center for Cosmology and Particle Physics}
\newline
\makebox[\mylongtab][l]{}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bf New York University}}
    \hfill
    \makebox[1in][r]{{\small \it Sep 2005--Sep 2008}}
\newline
\makebox[\mytab][l]{}{Postdoctoral Fellow, Kavli Institute for Cosmological Physics}
	\newline
\makebox[\mylongtab][l]{}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bf University of Chicago}}
	\hfill
	\makebox[1in][r]{{\small \it Aug 2002--Aug 2005}}
\newline
\makebox[\mytab][l]{}{Research Assistant, {\bf FNAL}}
	\hfill
	\makebox[1in][r]{\small \it Jun 1998--Sep 1998}
\newline
\tsp
\noindent
\makebox[\mytab][l]{}{Ph.D., {\bf University of Michigan}}
    \hfill
    \makebox[1in][r]{\small \it Sep 1997--Aug 2002}
\newline
\tsp
\makebox[\mytab][l]{}{B.S., {\bf University of Missouri}}
\hfill
\makebox[1in][r]{\small \it Aug 1992 -- May 1997}


%
% Experience...
%

\vspace{0.4cm}
\noindent
\makebox[1.25in][l]{{\large \bf Experience}}{}
\newline
\vspace{0.2cm}
\makebox[\mytab][l]{}{Physicist, {\bf Brookhaven National Laboratory}}
\newline
\makebox[1.25in][l]{}{}
        \hfill
        \makebox[1in][r]{{\small \it September 2008--present}}

\vspace{0.3cm}
\noindent
\makebox[1.25in][l]{{\large \bf Graduate and Postdoctoral Advisors and Advisees}}{}
\vspace{0.2cm}
\newline
\makebox[\myshorttab][l]{}{{\large \bf Advisors}}
\newline
\makebox[\mytab][l]{}{Postdoctoral Advisor: David Hogg, New York University}
\newline
\makebox[\mytab][l]{}{Postdoctoral Advisor: Josh Frieman, University of Chicago}
\newline
\makebox[\mytab][l]{}{Thesis Advisor: Timothy McKay, University of Michigan}
\newline
\makebox[\myshorttab][l]{}{{\large \bf Advisees}}
\newline
\makebox[\mytab][l]{}{Postdoc, BNL: Zhaoming Ma}
\newline
\makebox[\mytab][l]{}{Postdoc, BNL: Andres Plazas}


%\noindent
%\makebox[1.25in][l]{}
%\parbox{5.40in}{
%Ph.D., Physics\newline
%Thesis advisor: Prof. Timothy McKay\newline
%Title of thesis: ``Galaxies, Luminosity, and Mass: Gravitational Lensing Measurements of the Correlation between Dark and Luminous Matter''
%}




\newpage
\vspace{0.2in}
\noindent
\newline
\newline
{\Large {\bf Selected Publications for Erin Sheldon} }
\newline
Note Appendix 3 holds the references for the narrative. 
\vspace{4mm}

\begin{tabular}{p{3mm} p{5.5in}}

1 & E.~S. {Sheldon} et~al.
\newblock {Photometric Redshift Probability Distributions for Galaxies in the SDSS DR8}.
\newblock {\em \apjs}, 201 32, August 2012. \\[6pt]

2 & J. {Tinker}, E.~S. {Sheldon} et~al.
\newblock {Cosmological Constraints from Galaxy Clustering and the Mass-to-Number Ratio of Galaxy Clusters}.
\newblock {\em \apj},  745:16 January 2012. \\[6pt]

3 & E. {Rozo} et~al.
\newblock {Cosmological Constraints from the Sloan Digital Sky Survey maxBCG Cluster Catalog}.
\newblock {\em \apj}, 708:645-660, January 2010. \\[6pt]

4 & E.~S. {Sheldon} et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters III:
  Mass-to-light Ratios}.
\newblock {\em \apj}, 703:2232-2248, October 2009. \\[6pt]

5 & D.~E. {Johnston}, E.~S. {Sheldon}, et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters II: Cluster
  Density Profiles and the Mass--Richness Relation}.
\newblock {\em arXiv:0709.1159}, September 2007. \\[6pt]

6 & E.~S. {Sheldon} et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters I:
  Measurements}.
\newblock {\em \apj}, 703:2217-2231, October 2009. \\[6pt]

%5 & B.~P. {Koester} et~al.
%\newblock {A MaxBCG Catalog of 13,823 Galaxy Clusters from the Sloan Digital
%  Sky Survey}.
%\newblock {\em \apj}, 660:239--255, May 2007.\\[6pt]

7 & E.~S. {Sheldon} et~al.
\newblock {The Galaxy-Mass Correlation Function Measured from Weak Lensing in
  the Sloan Digital Sky Survey}.
\newblock {\em \aj}, 127:2544--2564, May 2004.\\[6pt]

%7 & T.~A. {McKay}, E.~S. {Sheldon}, D.~{Johnston}, E.~K. {Grebel}, F.~{Prada},
%  H.-W. {Rix}, N.~A. {Bahcall}, J.~{Brinkmann}, I.~{Csabai}, M.~{Fukugita},
%  D.~Q. {Lamb}, and D.~G. {York}.
%\newblock {Dynamical Confirmation of Sloan Digital Sky Survey Weak-lensing
%  Scaling Laws}.
%\newblock {\em \apjl}, 571:L85--L88, June 2002.\\[6pt]

8 & T.~A. {McKay}, E.~S. {Sheldon}, et~al.
\newblock {Galaxy Mass and Luminosity Scaling Laws Determined by Weak
  Gravitational Lensing}.
\newblock {\em ArXiv Astrophysics e-prints}, August 2001.\\[6pt]

9 & E.~S. {Sheldon} et~al.
\newblock {Weak-Lensing Measurements of 42 SDSS/RASS Galaxy Clusters}.
\newblock {\em \apj}, 554:881--887, June 2001.\\[6pt]

10 & P.~{Fischer}, T.~A. Mckay, E.~S. Sheldon, et~al.
\newblock {Weak Lensing with Sloan Digital Sky Survey Commissioning Data: The
  Galaxy-Mass Correlation Function to 1 Mpc}.
\newblock {\em \aj}, 120:1198--1208, September 2000.

\end{tabular}

\ssp
\ssp
\noindent
\parbox[l]{1.25in}{{\bf Synergistic \\ Activities}}
\parbox[t]{5.40in}{
Elected {\bf Builder} of the Dark Energy Survey \hfill {\small 2011} \newline
Elected {\bf Architect} of the Sloan Digital Sky Survey III \hfill {\small 2011} \newline
}

\newpage

\vspace{0.2in}
\noindent
\newline
\newline
{\Large {\bf Selected Collaborators} }
\newline

\noindent
\input{collab.tex}


\newpage
\addcontentsline{toc}{section}{Appendix 2: Current and Pending Support}
\section*{Appendix 2: Current and Pending Support}

Dr. Erin S. Sheldon is fully supported by Brookhaven National Laboratory as a
Physicist in the Astrophysics and Cosmology Group of the Physics
Department at 100\%.  Pending project approval, this support will be reduced
appropriately and redirected to the work proposed herein.  There is no other
support pending.

\begin{table}[h]
\begin{center}
\begin{tabular*}{0.85\textwidth}{ll}
B\&R \#YN010000  & \parbox[t]{\textwidth}{BNL Laboratory Directed 
   Research \& Development \\ Award} \\
                 & LDRD 10-45 (FY 2010 -â€“ FY 2012) \\
                 & Astrophysics \& Cosmology Initiative \\
                 & 100\%
\end{tabular*}
\parbox{0.85\textwidth}{\caption{{\bf Current Funding}: Program Development 
\& Lab Directed Research \& Development \label{table:support}}}
\end{center}
\end{table}



\newpage
\addcontentsline{toc}{section}{Appendix 3: Bibliography for Narrative}
\renewcommand{\refname}{\section*{Appendix 3: Bibliography for Narrative}\label{app:bib}}
\bibliographystyle{unsrt}
\bibliography{astroref}
\vspace{5mm}
\noindent
{\bf Key:} {\it AJ} is The Astronomical Journal, {\it ApJ} is The 
Astrophysical Journal and {\it MNRAS} is The Monthly Notices of the Royal
Astronomical Society.






\newpage
\addcontentsline{toc}{section}{Appendix 4: Facilities and Other Resources}
\section*{Appendix 4: Facilities and Other Resources}

We plan to acquire a moderate amount of computing.  The housing, power and
cooling, administration, and maintenance for these computers will be provided
by the RHIC-Atlas Computing Facility at Brookhaven National Lab at no
additional cost to this experiment.		

\newpage
\addcontentsline{toc}{section}{Appendix 5: Equipment}
\section*{Appendix 5: Equipment}

The equipment {\bf currently} available to this project is shared time on a set
of 34 compute nodes and a file server purchased in 2009-2012.  The file server
holds 40 TB. Nine of the compute nodes are 12 core with 32GB ram, twelve are 12
core with 48GB ram, and the rest are 8 core 32GB ram systems.  These are housed
at the RHIC-Atlas Computing Facility at BNL.  Because these are a highly
over-subscribed, shared resource, only a fraction of the resources can be
dedicated to this project.

% this is a pdf insert
\newpage
\addcontentsline{toc}{section}{Appendix 6: Other Attachments}
fake for letter

\end{document}
